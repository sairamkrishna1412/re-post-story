{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Hacker news data set\n",
    "* hacker news is a popular site where technology related stories and posts are voted and commented upon similar to reddit.hacker newa is extermely popular in technology and start up circles, and the posts that make it to the top of hacker listings can get hundreds of thousands of visitors as a result.\n",
    "* In this project we will work with a data set of subbmissions to popular techology site [hacker news.](https://news.ycombinator.com/)\n",
    "* You can find the data set [here](https://www.kaggle.com/hacker-news/hacker-news-posts), but note that it has been reduced from almost 300,000 rows to approximately 20,000 rows by removing all submissions that did not receive any comments, and then randomly sampling from the remaining submissions.\n",
    "* We're specifically interested in posts whose titles begin with either Ask HN or Show HN. Users submit Ask HN posts to ask the Hacker News community a specific question. and users submit Show HN posts to show the Hacker News community a project, product, or just generally something interesting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# introduction:\n",
    "* first we will open the hacker_news.csv file and make it a list of lists and remove the header row from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from csv import reader\n",
    "opened_file=open(\"hacker_news.csv\")\n",
    "read_file=reader(opened_file)\n",
    "hn=list(read_file)\n",
    "header=hn[0]\n",
    "hn=hn[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'title', 'url', 'num_points', 'num_comments', 'author', 'created_at']\n",
      "\n",
      "\n",
      "['12224879', 'Interactive Dynamic Video', 'http://www.interactivedynamicvideo.com/', '386', '52', 'ne0phyte', '8/4/2016 11:52']\n",
      "['10975351', 'How to Use Open Source and Shut the Fuck Up at the Same Time', 'http://hueniverse.com/2016/01/26/how-to-use-open-source-and-shut-the-fuck-up-at-the-same-time/', '39', '10', 'josep2', '1/26/2016 19:30']\n",
      "['11964716', \"Florida DJs May Face Felony for April Fools' Water Joke\", 'http://www.thewire.com/entertainment/2013/04/florida-djs-april-fools-water-joke/63798/', '2', '1', 'vezycash', '6/23/2016 22:20']\n",
      "['11919867', 'Technology ventures: From Idea to Enterprise', 'https://www.amazon.com/Technology-Ventures-Enterprise-Thomas-Byers/dp/0073523429', '3', '1', 'hswarna', '6/17/2016 0:01']\n",
      "['10301696', 'Note by Note: The Making of Steinway L1037 (2007)', 'http://www.nytimes.com/2007/11/07/movies/07stein.html?_r=0', '8', '2', 'walterbell', '9/30/2015 4:12']\n"
     ]
    }
   ],
   "source": [
    "print(header)\n",
    "print('\\n')\n",
    "for i in hn[:5]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seperating the data set into 3 parts:\n",
    "* the data set which we are given is divided into three parts :\n",
    "1. ask: it stores all the post titles beginning with ASK HN.\n",
    "2. show: it stores all the post titles beginning with SHOW HN.\n",
    "3. other: it stores other files which dont have either ASK HN or SHOW HN in their titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1744\n",
      "1162\n",
      "17194\n"
     ]
    }
   ],
   "source": [
    "ask=[]\n",
    "show=[]\n",
    "other=[]\n",
    "for i in hn:\n",
    "    title=i[1]\n",
    "    title_l=title.lower()\n",
    "    if title_l.startswith(\"ask hn\")==True:\n",
    "        ask.append(i)\n",
    "    elif title_l.startswith(\"show hn\")==True:\n",
    "        show.append(i)\n",
    "    else:\n",
    "        other.append(i)\n",
    "print(len(ask))\n",
    "print(len(show))\n",
    "print(len(other))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* we will now print the first five rows of ask list and you can observe that all the titles in this list start with ASK HN.\n",
    "* we will also going to print the first five rows of show list and all the titles start with SHOW HN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12296411', 'Ask HN: How to improve my personal website?', '', '2', '6', 'ahmedbaracat', '8/16/2016 9:55']\n",
      "['10610020', 'Ask HN: Am I the only one outraged by Twitter shutting down share counts?', '', '28', '29', 'tkfx', '11/22/2015 13:43']\n",
      "['11610310', 'Ask HN: Aby recent changes to CSS that broke mobile?', '', '1', '1', 'polskibus', '5/2/2016 10:14']\n",
      "['12210105', 'Ask HN: Looking for Employee #3 How do I do it?', '', '1', '3', 'sph130', '8/2/2016 14:20']\n",
      "['10394168', 'Ask HN: Someone offered to buy my browser extension from me. What now?', '', '28', '17', 'roykolak', '10/15/2015 16:38']\n",
      "\n",
      "\n",
      "['10627194', 'Show HN: Wio Link  ESP8266 Based Web of Things Hardware Development Platform', 'https://iot.seeed.cc', '26', '22', 'kfihihc', '11/25/2015 14:03']\n",
      "['10646440', 'Show HN: Something pointless I made', 'http://dn.ht/picklecat/', '747', '102', 'dhotson', '11/29/2015 22:46']\n",
      "['11590768', 'Show HN: Shanhu.io, a programming playground powered by e8vm', 'https://shanhu.io', '1', '1', 'h8liu', '4/28/2016 18:05']\n",
      "['12178806', 'Show HN: Webscope  Easy way for web developers to communicate with Clients', 'http://webscopeapp.com', '3', '3', 'fastbrick', '7/28/2016 7:11']\n",
      "['10872799', 'Show HN: GeoScreenshot  Easily test Geo-IP based web pages', 'https://www.geoscreenshot.com/', '1', '9', 'kpsychwave', '1/9/2016 20:45']\n"
     ]
    }
   ],
   "source": [
    "for i in ask[:5]:\n",
    "    print(i)\n",
    "print('')\n",
    "print('')\n",
    "for j in show[:5]:\n",
    "    print(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## determining the average number of comments in ask and show lists .\n",
    "* to find the average number of comments in ask and show lists we need to first initiate total_ask_c and total_show_c to 0\n",
    "* loop through the ask list, number of comments is in the fifth coloumn in ask list we need to get the element at index 4 in each row. dont forget to convert the value to integer type to perform addition to it.\n",
    "* calculate comments and assign it to variable total_ask_c .\n",
    "* similarly do the same operations on show list and find the total_show_c\n",
    "* after that calculate average comments for both the lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total ask comments:   24483\n",
      "total show comments:   11988\n",
      "average ask comments:   14.038417431192661\n",
      "average show comments:  10.31669535283993\n"
     ]
    }
   ],
   "source": [
    "total_ask_c=0\n",
    "total_show_c=0\n",
    "for i in ask:\n",
    "    comments_a=int(i[4])\n",
    "    total_ask_c+=comments_a\n",
    "for i in show:\n",
    "    comments_s=int(i[4])\n",
    "    total_show_c+=comments_s\n",
    "print(\"total ask comments:  \",total_ask_c)\n",
    "print(\"total show comments:  \",total_show_c)\n",
    "average_ask_c=total_ask_c/len(ask)\n",
    "average_show_c=total_show_c/len(show)\n",
    "print(\"average ask comments:  \",average_ask_c)\n",
    "print(\"average show comments: \",average_show_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function method to find the average comments: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def average(lis,index):\n",
    "    total_c=0\n",
    "    for i in lis:\n",
    "        comments=int(i[index])\n",
    "        total_c+=comments\n",
    "        average_c=total_c/len(lis)\n",
    "    print(\"total comments:  \",total_c)\n",
    "    print(\"average comments:  \",average_c)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total comments:   24483\n",
      "average comments:   14.038417431192661\n"
     ]
    }
   ],
   "source": [
    "average(ask,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total comments:   24483\n",
      "average comments:   14.038417431192661\n"
     ]
    }
   ],
   "source": [
    "average(ask,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* now we will create 2 dictionaries which will store counts and comments per hour\n",
    "* first import datetime module as dt because we will need it to convert the created_at coloumn at index 6 in hour.\n",
    "* then create an empty list named result list.which we will be a list of lists\n",
    "* append the date and comments to the list. here date and comments form for each row form a list and stored into the bigger list result_list\n",
    "* create 2 dictionaries counts_by_hour and comments_by_hour which will be a frequency table where the key is hour and the value is number of posts , number of comments for that hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'19': 110, '21': 109, '01': 60, '11': 58, '13': 85, '06': 44, '04': 47, '14': 107, '10': 59, '03': 54, '07': 34, '18': 109, '17': 100, '12': 73, '08': 48, '20': 80, '05': 46, '16': 108, '15': 116, '09': 45, '00': 55, '02': 58, '22': 71, '23': 68}\n",
      "{'19': 1188, '21': 1745, '01': 683, '11': 641, '13': 1253, '06': 397, '04': 337, '14': 1416, '10': 793, '03': 421, '07': 267, '18': 1439, '17': 1146, '12': 687, '08': 492, '20': 1722, '05': 464, '16': 1814, '15': 4477, '09': 251, '00': 447, '02': 1381, '22': 479, '23': 543}\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "result_list=[]\n",
    "for i in ask:\n",
    "    result_list.append([i[6],int(i[4])])#i[6]=created_at , i[4]=comments\n",
    "counts_by_hour={}\n",
    "comments_by_hour={}\n",
    "date_format = \"%m/%d/%Y %H:%M\"\n",
    "for i in result_list:\n",
    "    date=i[0]\n",
    "    comment=i[1]\n",
    "    hour=dt.datetime.strptime(date, date_format).strftime(\"%H\")\n",
    "    if hour in counts_by_hour:\n",
    "        counts_by_hour[hour]+=1\n",
    "        comments_by_hour[hour]+=comment\n",
    "    else:\n",
    "        counts_by_hour[hour]=1\n",
    "        comments_by_hour[hour]=comment\n",
    "print(counts_by_hour)\n",
    "print(comments_by_hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* we will now create a list which store lists within it and each sublist is of 2 elements where the first element is hour and the second element is average comments for that hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['19', 10.8], ['21', 16.009174311926607], ['01', 11.383333333333333], ['11', 11.051724137931034], ['13', 14.741176470588234], ['06', 9.022727272727273], ['04', 7.170212765957447], ['14', 13.233644859813085], ['10', 13.440677966101696], ['03', 7.796296296296297], ['07', 7.852941176470588], ['18', 13.20183486238532], ['17', 11.46], ['12', 9.41095890410959], ['08', 10.25], ['20', 21.525], ['05', 10.08695652173913], ['16', 16.796296296296298], ['15', 38.5948275862069], ['09', 5.5777777777777775], ['00', 8.127272727272727], ['02', 23.810344827586206], ['22', 6.746478873239437], ['23', 7.985294117647059]]\n"
     ]
    }
   ],
   "source": [
    "avg_by_hour=[]\n",
    "for hr in comments_by_hour:\n",
    "    avg_by_hour.append([hr,comments_by_hour[hr]/counts_by_hour[hr]])\n",
    "print(avg_by_hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sorting and printing values from a list of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10.8, '19'], [16.009174311926607, '21'], [11.383333333333333, '01'], [11.051724137931034, '11'], [14.741176470588234, '13'], [9.022727272727273, '06'], [7.170212765957447, '04'], [13.233644859813085, '14'], [13.440677966101696, '10'], [7.796296296296297, '03'], [7.852941176470588, '07'], [13.20183486238532, '18'], [11.46, '17'], [9.41095890410959, '12'], [10.25, '08'], [21.525, '20'], [10.08695652173913, '05'], [16.796296296296298, '16'], [38.5948275862069, '15'], [5.5777777777777775, '09'], [8.127272727272727, '00'], [23.810344827586206, '02'], [6.746478873239437, '22'], [7.985294117647059, '23']]\n",
      "\n",
      "\n",
      "sorted order: \n",
      "[[38.5948275862069, '15'], [23.810344827586206, '02'], [21.525, '20'], [16.796296296296298, '16'], [16.009174311926607, '21'], [14.741176470588234, '13'], [13.440677966101696, '10'], [13.233644859813085, '14'], [13.20183486238532, '18'], [11.46, '17'], [11.383333333333333, '01'], [11.051724137931034, '11'], [10.8, '19'], [10.25, '08'], [10.08695652173913, '05'], [9.41095890410959, '12'], [9.022727272727273, '06'], [8.127272727272727, '00'], [7.985294117647059, '23'], [7.852941176470588, '07'], [7.796296296296297, '03'], [7.170212765957447, '04'], [6.746478873239437, '22'], [5.5777777777777775, '09']]\n"
     ]
    }
   ],
   "source": [
    "swap_avg_by_hour=[]\n",
    "for i in avg_by_hour:\n",
    "    swap_avg_by_hour.append([i[1],i[0]])\n",
    "print(swap_avg_by_hour)\n",
    "sorted_swap=sorted(swap_avg_by_hour,reverse=True)\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"sorted order: \")\n",
    "print(sorted_swap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:00 : 38.59 average comments per post.\n",
      "02:00 : 23.81 average comments per post.\n",
      "20:00 : 21.52 average comments per post.\n",
      "16:00 : 16.80 average comments per post.\n",
      "21:00 : 16.01 average comments per post.\n"
     ]
    }
   ],
   "source": [
    "for i in sorted_swap[:5]:\n",
    "    print(\"{}:00 : {:.2f} average comments per post.\".format(i[1],i[0]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The hour that receives the most comments per post on average is 15:00, with an average of 38.59 comments per post. There's about a 60% increase in the number of comments between the hours with the highest and second highest average number of comments.\n",
    "\n",
    "* According to the data set documentation, the timezone used is Eastern Time in the US. So, we could also write 15:00 as 3:00 pm est."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# conclusions \n",
    "* in this project our aim was to find which of the two ask posts or show posts more likely received comments and we found out that ask posts received more comments as the average comments for ask posts was 14.03 compared to average show comments was 10.31\n",
    "* Then we had to find the hour of the day for which ask posts had maximum average comments and it was the 15th hour of the day.The timezone used is eastetrn time in the US. So, we could write 15:00 as 3:00 pm est. which converted to time in india is 12:30 am in the morning which looks like an inapporiate time for maximum comments for ask posts in india."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
